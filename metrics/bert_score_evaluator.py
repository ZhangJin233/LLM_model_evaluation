from bert_score import score as bert_score_fn
from bert_score import BERTScorer
import torch

def calculate_bert_score(candidate, reference, lang="en", model_type=None, rescale_with_baseline=True):
    """
    è®¡ç®—ä¸€ä¸ªå€™é€‰æ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬ä¹‹é—´çš„BERTScore
    
    Args:
        candidate (str): å€™é€‰æ–‡æœ¬
        reference (str): å‚è€ƒæ–‡æœ¬
        lang (str): è¯­è¨€ä»£ç 
        model_type (str): ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ç±»å‹
        rescale_with_baseline (bool): æ˜¯å¦ä½¿ç”¨åŸºçº¿åˆ†æ•°è¿›è¡Œç¼©æ”¾
        
    Returns:
        tuple: (precision, recall, f1) å…ƒç»„
    """
    # ä¸ºä¸­æ–‡é€‰æ‹©åˆé€‚çš„é»˜è®¤æ¨¡å‹
    if model_type is None and lang == "zh":
        model_type = "bert-base-chinese"
    
    P, R, F1 = bert_score_fn(
        [candidate], 
        [reference], 
        lang=lang, 
        model_type=model_type,
        rescale_with_baseline=rescale_with_baseline,
        verbose=False
    )
    return P.item(), R.item(), F1.item()

def print_bert_score_results(candidate, reference, lang="en", model_type=None, rescale_with_baseline=True):
    """
    è®¡ç®—å¹¶æ‰“å°BERTScoreç»“æœ
    
    Args:
        candidate (str): å€™é€‰æ–‡æœ¬
        reference (str): å‚è€ƒæ–‡æœ¬
        lang (str): è¯­è¨€ä»£ç 
        model_type (str): ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ç±»å‹
        rescale_with_baseline (bool): æ˜¯å¦ä½¿ç”¨åŸºçº¿åˆ†æ•°è¿›è¡Œç¼©æ”¾
    
    Returns:
        float: F1åˆ†æ•°
    """
    p, r, f1 = calculate_bert_score(candidate, reference, lang, model_type, rescale_with_baseline)
    print("=== BERTScore ===")
    print(f"å€™é€‰æ–‡æœ¬: {candidate}")
    print(f"å‚è€ƒæ–‡æœ¬: {reference}")
    if rescale_with_baseline:
        print(f"ç²¾ç¡®ç‡(ç¼©æ”¾å): {p:.4f}, å¬å›ç‡(ç¼©æ”¾å): {r:.4f}, F1(ç¼©æ”¾å): {f1:.4f}")
    else:
        print(f"ç²¾ç¡®ç‡: {p:.4f}, å¬å›ç‡: {r:.4f}, F1: {f1:.4f}")
    return f1

class BERTScoreEvaluator:
    """
    BERTScoreè¯„ä¼°å™¨ï¼Œé€‚ç”¨äºå¤šæ¬¡è¯„ä¼°ä»¥èŠ‚çœæ¨¡å‹åŠ è½½æ—¶é—´
    """
    def __init__(self, lang="zh", model_type=None, rescale_with_baseline=True):
        """
        åˆå§‹åŒ–BERTScoreè¯„ä¼°å™¨
        
        Args:
            lang (str): è¯­è¨€ä»£ç 
            model_type (str): ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ç±»å‹
            rescale_with_baseline (bool): æ˜¯å¦ä½¿ç”¨åŸºçº¿åˆ†æ•°è¿›è¡Œç¼©æ”¾
        """
        # ä¸ºä¸­æ–‡é€‰æ‹©åˆé€‚çš„é»˜è®¤æ¨¡å‹
        if model_type is None and lang == "zh":
            model_type = "bert-base-chinese"
            
        self.scorer = BERTScorer(
            lang=lang, 
            model_type=model_type,
            rescale_with_baseline=rescale_with_baseline
        )
        self.lang = lang
        self.model_type = model_type
        self.rescale_with_baseline = rescale_with_baseline
    
    def score(self, candidates, references):
        """
        è®¡ç®—å¤šä¸ªå€™é€‰æ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬ä¹‹é—´çš„BERTScore
        
        Args:
            candidates (list): å€™é€‰æ–‡æœ¬åˆ—è¡¨
            references (list): å‚è€ƒæ–‡æœ¬åˆ—è¡¨
            
        Returns:
            tuple: (precision, recall, f1) å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå¼ é‡
        """
        if isinstance(candidates, str):
            candidates = [candidates]
        if isinstance(references, str):
            references = [references]
        
        return self.scorer.score(candidates, references)
    
    def evaluate_rag_response(self, response, reference):
        """
        è¯„ä¼°RAGç³»ç»Ÿçš„å“åº”ä¸å‚è€ƒç­”æ¡ˆçš„åŒ¹é…ç¨‹åº¦
        
        Args:
            response (str): RAGç³»ç»Ÿçš„å“åº”
            reference (str): å‚è€ƒç­”æ¡ˆ
            
        Returns:
            dict: åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸
        """
        P, R, F1 = self.score([response], [reference])
        
        result = {
            "precision": P.item(),
            "recall": R.item(),
            "f1": F1.item()
        }
        
        # æ·»åŠ è´¨é‡è¯„ä¼°
        if result["f1"] > 0.8:
            result["quality"] = "é«˜"
            result["description"] = "âœ… å›ç­”è´¨é‡å¾ˆé«˜ï¼Œä¸å‚è€ƒç­”æ¡ˆè¯­ä¹‰ç›¸ä¼¼åº¦é«˜"
        elif result["f1"] > 0.6:
            result["quality"] = "ä¸­"
            result["description"] = "ğŸŸ¡ å›ç­”è´¨é‡ä¸é”™ï¼Œä½†ä¸å‚è€ƒç­”æ¡ˆæœ‰ä¸€å®šå·®å¼‚"
        else:
            result["quality"] = "ä½"
            result["description"] = "âŒ å›ç­”è´¨é‡è¾ƒå·®ï¼Œä¸å‚è€ƒç­”æ¡ˆå·®å¼‚è¾ƒå¤§"
            
        return result

if __name__ == "__main__":
    # ç®€å•çš„ç¤ºä¾‹
    candidate = "AeroBot V2 æ— äººæœºçš„ç»­èˆªæ—¶é—´æ˜¯ 35 åˆ†é’Ÿï¼Œæœ€å¤§å›¾ä¼ è·ç¦»ä¸º 15 å…¬é‡Œã€‚"
    reference = "æ ‡å‡†ç”µæ± å¯æä¾›é•¿è¾¾ 35 åˆ†é’Ÿçš„é£è¡Œæ—¶é—´ã€‚é‡‡ç”¨æœ€æ–°çš„ SkyLink 3.0 æŠ€æœ¯ï¼Œæœ€å¤§å›¾ä¼ è·ç¦»ä¸º 15 å…¬é‡Œã€‚"
    
    # ä½¿ç”¨å•æ¬¡è¯„ä¼°å‡½æ•°
    print("=== å•æ¬¡è¯„ä¼° ===")
    print_bert_score_results(candidate, reference, "zh", rescale_with_baseline=True)
    
    # ä½¿ç”¨è¯„ä¼°å™¨è¿›è¡Œå¤šæ¬¡è¯„ä¼°ï¼ˆæ›´é«˜æ•ˆï¼‰
    print("\n=== ä½¿ç”¨è¯„ä¼°å™¨ ===")
    evaluator = BERTScoreEvaluator(lang="zh")
    result = evaluator.evaluate_rag_response(candidate, reference)
    print(f"ç²¾ç¡®ç‡: {result['precision']:.4f}, å¬å›ç‡: {result['recall']:.4f}, F1: {result['f1']:.4f}")
    print(result["description"])
    
    # å¦ä¸€ä¸ªç¤ºä¾‹
    candidate2 = "è¿™æ¬¾æ— äººæœºå¯ä»¥åœ¨5çº§é£ä¸‹ç¨³å®šé£è¡Œï¼Œä½†æ˜¯ä¸é˜²æ°´ï¼Œä¸å»ºè®®åœ¨æµ·è¾¹ä½¿ç”¨ã€‚"
    reference2 = "AeroBot V2å¯ä»¥åœ¨5çº§é£ä¸‹ç¨³å®šé£è¡Œï¼Œä½†äº§å“ä¸å…·å¤‡é˜²æ°´åŠŸèƒ½ï¼Œè¯·é¿å…åœ¨é›¨å¤©é£è¡Œã€‚"
    
    result2 = evaluator.evaluate_rag_response(candidate2, reference2)
    print("\nå€™é€‰æ–‡æœ¬:", candidate2)
    print("å‚è€ƒæ–‡æœ¬:", reference2)
    print(f"ç²¾ç¡®ç‡: {result2['precision']:.4f}, å¬å›ç‡: {result2['recall']:.4f}, F1: {result2['f1']:.4f}")
    print(result2["description"])
